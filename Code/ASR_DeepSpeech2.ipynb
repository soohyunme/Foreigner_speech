{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASR_DeepSpeech2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soohyunme/Foreigner_speech/blob/main/Code/ASR_DeepSpeech2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbFBOE8uwJ7V"
      },
      "source": [
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install hangul_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0NSJgqnCo1an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNuwbt4KDYEt"
      },
      "source": [
        "# 모듈 설치 및 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeSjAhuVnZT6"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from scipy.io import wavfile\n",
        "from collections import defaultdict, Counter\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from hangul_utils import join_jamos\n",
        "\n",
        "from typing import Tuple, Union\n",
        "\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try:\n",
        "#   resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "\n",
        "#   tf.config.experimental_connect_to_cluster(resolver)\n",
        "#   tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#   strategy = tf.distribute.TPUStrategy(resolver)\n",
        "# except:\n",
        "#   print('TPU device not found')"
      ],
      "metadata": {
        "id": "vUSjPRDppCIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTi9cpDOeuHj"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print('GPU device not found')\n",
        "else:\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAjNysU_0yE2"
      },
      "source": [
        "# 경로 설정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/03.foreigner_speech/dataset'"
      ],
      "metadata": {
        "id": "O0YyjdNN21Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ3JILIr4fGH"
      },
      "source": [
        "# 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2H8e8EUzl4V"
      },
      "source": [
        "train_paths = glob(data_path+\"/train/wav/*.wav\")\n",
        "# json_paths = glob(data_path+\"/Train/JSON/*.json\")\n",
        "test_paths = glob(data_path+\"/test/*.wav\")\n",
        "text_df = pd.read_csv(data_path+'/final_text.csv')[['fileName','new_ReadingLabelText']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foC6NPl2eAXm"
      },
      "source": [
        "## 레이블 df에서 존재하는 데이터만 선택"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = []\n",
        "for each in train_paths:\n",
        "  file_list.append(each.replace(data_path+\"/train/wav/\",''))\n",
        "file_df = pd.DataFrame(file_list,columns=['fileName'])"
      ],
      "metadata": {
        "id": "BambE61pz7DB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_df = text_df.merge(file_df)\n",
        "len(text_df)"
      ],
      "metadata": {
        "id": "a3yuGRh4N1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRPrwWTTd_pe"
      },
      "source": [
        "def train_to_dataset(df):\n",
        "  tuple_list = []\n",
        "  for i in tqdm(range(len(df))):\n",
        "    file_name, utterance = df.iloc[i,0], df.iloc[i,1]\n",
        "    waveform, sr = torchaudio.load(data_path+'/train/wav/'+file_name)\n",
        "    waveform = waveform[0].reshape(1,-1)\n",
        "    tuple_data = (waveform, sr,utterance,0 , 0, 0)\n",
        "    tuple_list.append(tuple_data)\n",
        "    result = list(tuple_list)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test_to_dataset(df):\n",
        "#   tuple_list = []\n",
        "#   for i in tqdm(range(len(df))):\n",
        "#     file_name = df.iloc[i,0]\n",
        "#     waveform, sr = torchaudio.load(data_path+'./test/'+file_name)\n",
        "#     waveform = waveform[0].reshape(1,-1)\n",
        "#     tuple_data = (waveform, sr,'',0 , 0, 0)\n",
        "#     tuple_list.append(tuple_data)\n",
        "#     result = list(tuple_list)\n",
        "#   return result"
      ],
      "metadata": {
        "id": "ipErcNC79j67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_to_dataset(df):\n",
        "  tuple_list = []\n",
        "  for i in tqdm(range(len(df))):\n",
        "    file_name = df[i]\n",
        "    waveform, sr = torchaudio.load(file_name)\n",
        "    waveform = waveform[0].reshape(1,-1)\n",
        "    tuple_data = (waveform, sr,'',0 , 0, 0)\n",
        "    tuple_list.append(tuple_data)\n",
        "    result = list(tuple_list)\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "ahG16HgHqOYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len_train = int(len(text_df)*0.8)"
      ],
      "metadata": {
        "id": "yYJTpbDk4RXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = train_to_dataset(text_df.iloc[:len_train]) \n",
        "# train_dataset = train_to_dataset(text_df) \n",
        "train_dataset[0]"
      ],
      "metadata": {
        "id": "yzukakWD9ryp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset = train_to_dataset(text_df.iloc[len_train:]) \n",
        "# valid_dataset = test_to_dataset(test_paths) \n",
        "valid_dataset[0]"
      ],
      "metadata": {
        "id": "jUJ9H7Y4P41C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.DataFrame(test_paths,columns=['fileName'])"
      ],
      "metadata": {
        "id": "1tQ_FbvK9zNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ★★★★★★★★★ iloc 지우기 ★★★★★★★★★★★\n",
        "test_dataset = test_to_dataset(test_df['fileName']) \n",
        "test_dataset[0]"
      ],
      "metadata": {
        "id": "37SswpnoHIya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 처리"
      ],
      "metadata": {
        "id": "vp2NyZZBXK7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INITIALS = list(\"ㄱㄲㄴㄷㄸㄹㅁㅂㅃㅅㅆㅇㅈㅉㅊㅋㅌㅍㅎ\")\n",
        "\"char list: Hangul initials (초성)\"\n",
        "\n",
        "MEDIALS = list(\"ㅏㅐㅑㅒㅓㅔㅕㅖㅗㅘㅙㅚㅛㅜㅝㅞㅟㅠㅡㅢㅣ\")\n",
        "\"char list: Hangul medials (중성)\"\n",
        "\n",
        "FINALS = list(\"∅ㄱㄲㄳㄴㄵㄶㄷㄹㄺㄻㄼㄽㄾㄿㅀㅁㅂㅄㅅㅆㅇㅈㅊㅋㅌㅍㅎ\")\n",
        "\"char list: Hangul finals (종성).\"\n",
        "\n",
        "SPACE_TOKEN = \" \"\n",
        "LABELS = sorted({SPACE_TOKEN}.union(INITIALS).union(MEDIALS).union(FINALS))\n",
        "\n",
        "\"char list: All CTC labels.\"\n",
        "\n",
        "def check_syllable(char):\n",
        "  return 0xAC00 <= ord(char) <= 0xD7A3\n",
        "\n",
        "def split_syllable(char):\n",
        "  assert check_syllable(char)\n",
        "  diff = ord(char) - 0xAC00\n",
        "  _m = diff % 28\n",
        "  _d = (diff - _m) // 28\n",
        "  return (INITIALS[_d // 21], MEDIALS[_d % 21], FINALS[_m])\n",
        "\n",
        "def preprocess(str):\n",
        "  result = \"\"\n",
        "  for char in re.sub(\"\\\\s+\", SPACE_TOKEN, str.strip()):\n",
        "    if char == SPACE_TOKEN:\n",
        "      result += SPACE_TOKEN\n",
        "    elif check_syllable(char):\n",
        "      result += \"\".join(split_syllable(char))\n",
        "  return result\n",
        "\n",
        "def join_text(text):\n",
        "  result = text.replace('∅','')\n",
        "  result = join_jamos(result)\n",
        "  return result"
      ],
      "metadata": {
        "id": "KoOjmsI3Fvt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 오류 함수 정의"
      ],
      "metadata": {
        "id": "QZyRlkXOXaYo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlBQJ2QopRFk"
      },
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다. \n",
        "    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다. \n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru55tshPqejt"
      },
      "source": [
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n",
        "     : param reference : 참조 문장.\n",
        "     : param hypothesis : 가설 문장.\n",
        "     : param ignore_case : 대소 문자 구분 여부.\n",
        "     : param delimiter : 입력 문장의 구분자.\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-LCRpXVqhv8"
      },
      "source": [
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gBZ_3-Sqm5D"
      },
      "source": [
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER). \n",
        "    WER = (Sw + Dw + Iw) / Nw\n",
        "    Sw는 대체 된 단어의 수입니다.\n",
        "    Dw는 삭제 된 단어의 수입니다.\n",
        "    Iw는 삽입 된 단어의 수입니다.\n",
        "    Nw는 참조의 단어 수입니다.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6YUEk-2qqaw"
      },
      "source": [
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER). \n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text map"
      ],
      "metadata": {
        "id": "xYeZ9oV4Xe3e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS2jhIrBqs_G"
      },
      "source": [
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        ㅎ 2\n",
        "        ㅍ 3\n",
        "        ㅌ 4\n",
        "        ㅋ 5\n",
        "        ㅊ 6\n",
        "        ㅈ 7\n",
        "        ㅇ 8\n",
        "        ㅆ 9\n",
        "        ㅅ 10\n",
        "        ㅂ 11\n",
        "        ㅁ 12\n",
        "        ㄹ 13\n",
        "        ㄷ 14\n",
        "        ㄴ 15\n",
        "        ㄲ 16\n",
        "        ㄱ 17\n",
        "        ㅣ 18\n",
        "        ㅢ 19\n",
        "        ㅡ 20\n",
        "        ㅠ 21\n",
        "        ㅟ 22\n",
        "        ㅞ 23\n",
        "        ㅝ 24\n",
        "        ㅜ 25\n",
        "        ㅛ 26\n",
        "        ㅚ 27\n",
        "        ㅙ 28\n",
        "        ㅘ 29\n",
        "        ㅗ 30\n",
        "        ㅖ 31\n",
        "        ㅕ 32\n",
        "        ㅔ 33\n",
        "        ㅓ 34\n",
        "        ㅒ 35\n",
        "        ㅑ 36\n",
        "        ㅐ 37\n",
        "        ㅏ 38\n",
        "        ㅉ 39\n",
        "        ㅄ 40\n",
        "        ㅃ 41\n",
        "        ㅀ 42\n",
        "        ㄿ 43\n",
        "        ㄾ 44\n",
        "        ㄽ 45\n",
        "        ㄼ 46\n",
        "        ㄻ 47\n",
        "        ㄺ 48\n",
        "        ㄸ 49\n",
        "        ㄶ 50\n",
        "        ㄵ 51\n",
        "        ㄳ 52\n",
        "        ∅ 53\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')\n",
        "    def join_text(text):\n",
        "      result = text.replace('∅','')\n",
        "      result = join_jamos(result)\n",
        "      return result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform"
      ],
      "metadata": {
        "id": "I2pfQvS_XzXo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMUWPlL8qwQg"
      },
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "text_transform = TextTransform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing"
      ],
      "metadata": {
        "id": "XP459co36A1W"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrbecqsFq8qM"
      },
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'test':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(preprocess(utterance)))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder"
      ],
      "metadata": {
        "id": "5DhK9CYA5-M-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZVpzOyHq9hX"
      },
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=54, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets\n",
        "# main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "j1Gsm6a16G3-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXAYnQ4arD2P"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "# The two sets of activations are summed to form the output activations for the layer h The function g(·) can be the standard recurrent operation\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loader"
      ],
      "metadata": {
        "id": "5IXqys6V6KJI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMITaH3ALmr8"
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "def load_dataset(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, _, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "        \n",
        "    return spectrograms\n",
        "\n",
        "def get_dataloader(data):\n",
        "    x_train = load_dataset(data, \"train\")\n",
        "    x_valid = load_dataset(data, 'valid')\n",
        "\n",
        "    mean = np.mean(x_train)\n",
        "    std = np.std(x_train)\n",
        "    x_train = (x_train - mean)/std\n",
        "    x_valid = (x_valid - mean)/std\n",
        "\n",
        "    train_set = Dataset(x_train)\n",
        "    vaild_set = Dataset(x_valid)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(vaild_set, batch_size=4, shuffle=False, drop_last=False)\n",
        "    \n",
        "    return train_loader, valid_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train test"
      ],
      "metadata": {
        "id": "Ur-WZGVG6M9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        global spectrograms\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        iter_meter.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "Yp4Xc_IdwMSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            # The output layer L is a softmax computing a probability distribution over characters given\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "    \n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n"
      ],
      "metadata": {
        "id": "IzDZ5j_4tfsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test 함수를 변경해서 predict을 만들어야 하는걸까? -> 더 알아보기\n",
        "# 일단은 test함수에서 loss와 error 부분만 제거 후 결과값을 df로 만들어서 출력해보기\n",
        "\n",
        "def predict(model, device, test_loader, criterion):\n",
        "    print('\\predicting...')\n",
        "    result_text = []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            # The output layer L is a softmax computing a probability distribution over characters given\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            # loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            # test_loss += loss.item() / len(test_loader)\n",
        "            decoded_preds, _ = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            result_text.append(join_text(decoded_preds[0]))\n",
        "    result = pd.DataFrame(result_text,columns=['ReadingLableText'])\n",
        "    submission = pd.read_excel(data_path+'/test/submission.xlsx')\n",
        "    submission['ReadingLableText'] = result['ReadingLableText']\n",
        "\n",
        "    #         for j in range(len(decoded_preds)):\n",
        "    #             test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "    #             test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "    # avg_cer = sum(test_cer)/len(test_cer)\n",
        "    # avg_wer = sum(test_wer)/len(test_wer)\n",
        "    \n",
        "    # print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "    return submission\n"
      ],
      "metadata": {
        "id": "9-RVdUk23nQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "ms8WIr7J6QXO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omm1e-e4Mu0V"
      },
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 55,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    print(device)\n",
        "\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=valid_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "    if os.path.isfile(data_path + '/model/model_state_dict.pth'):\n",
        "        model.load_state_dict(torch.load(data_path + '/model/model_state_dict.pth'))\n",
        "        print('model load')\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=54).to(device)\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    # for epoch in range(1, epochs + 1):\n",
        "    #     train(model, device, train_loader, criterion, optimizer, epoch, iter_meter)\n",
        "    #     test(model, device, test_loader, criterion, epoch, iter_meter)\n",
        "    # torch.save(model.state_dict(), data_path + '/model/model_state_dict.pth')  # 모델 객체의 state_dict 저장\n",
        "    # print('model save')\n",
        "\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "    submission = predict(model, device, test_loader, criterion)\n",
        "    submission.to_excel(data_path+'/submission_sample.xlsx',index=False)\n",
        "    print('submission save')\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "learning_rate = 5e-4\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "\n",
        "main(learning_rate, batch_size, epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8cJk8mgODFAG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}